
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>CEVR</title>

    <style>
    .responsive
    {
    width: 100%
    }
    </style>

    <style type="text/css">
        /* Add a container class to center the table */
        .table-container {
            display: flex;
            justify-content: center;
            align-items: center;
            height: 90vh; /* Adjust the height if needed */
            margin-top: 0px;
        }

        .tg  {border-collapse:collapse;border-spacing:0;}
        .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
            overflow:hidden;padding:10px 5px;word-break:normal;}
        .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
            font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
        .tg .tg-eiei{background-color:#ffffff;border-color:#9b9b9b;color:#009901;font-weight:bold;text-align:center;vertical-align:middle}
        .tg .tg-7s0l{background-color:#ffffff;border-color:#9b9b9b;color:#000000;font-weight:bold;text-align:center;vertical-align:middle}
        .tg .tg-jb0k{background-color:#ffffff;border-color:#9b9b9b;color:#000000;text-align:center;vertical-align:middle}
    </style>


    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>âš™</text></svg>">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
    <script src="js/video_comparison.js"></script>
    <script src="js/result.js"></script>


</head>




<body>

    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>CEVR</b>: Learning Continuous Exposure Value
            </br>
                Representations for Single-Image HDR Reconstruction</br> 
                <small>
                ICCV 2023
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a>
                          Su-Kai Chen
                        </a>
                        </br>NYCU
                    </li>
                    <li>
                        <a>
                          Hung-Lin Yen
                        </a>
                        </br>NYCU
                    </li>
                    <li>
                        <a href="https://yulunalexliu.github.io/">
                          Yu-Lun Liu
                        </a>
                        </br>NYCU
                    </li>
                    <li>
                        <a href="https://minhungchen.netlify.app/">
                          Min-Hung Chen
                        </a>
                        </br>NVIDIA
                    </li>
                    <li>
                        <a href="https://eborboihuc.github.io/">
                          Hou-Ning Hu
                        </a>
                        </br>MediaTek
                    </li>
                    <li>
                        <a href="https://sites.google.com/g2.nctu.edu.tw/wpeng">
                          Wen-Hsiao Peng
                        </a>
                        </br>NYCU
                    </li>                    <li>
                        <a href="https://sites.google.com/site/yylinweb/">
                          Yen-Yu Lin
                        </a>
                        </br>NYCU
                    </li>
                    <!-- </br>NYCU -->
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-sm-6 col-sm-offset-3 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="http://vllab.cs.nctu.edu.tw/images/paper/iccv-chen23.pdf">
                            <image src="img/paper_thumbnail.jpg" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://www.youtube.com/watch?v=Az8W2lGegcg">
                            <image src="img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://drive.google.com/file/d/1xeCT3APYkTnxeotb_t0wxdSPzBLbnU_p/view?usp=sharing">
                            <image src="img/evaluation_results_teaser.jpg" height="60px">
                                <h4><strong>Evaluation</br>results</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</br>[coming soon]</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>
<br>

        <div class="section">
          <!-- Icon Section -->
          <div class="row center">
            <div class="col-md-8 col-md-offset-2">
              <div class="image-container">
                <img class="responsive-img" src="img/CEVR_Teaser.png" alt="Image Description">
              </div>
            </div>
          </div>
        </div>

<br>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
Deep learning is commonly used to produce impressive results in reconstructing HDR images from LDR images. LDR stack-based methods are used for single-image HDR reconstruction, generating an HDR image from a deep learning-generated LDR stack. However, current methods generate the LDR stack with predetermined exposure values (EVs), which may limit the quality of HDR reconstruction. To address this, we propose the continuous exposure value representation (CEVR) model, which uses an implicit function to generate LDR images with arbitrary EVs, including those unseen during training. Our flexible  pproach generates a continuous stack with more images containing diverse EVs, significantly improving HDR reconstruction. We use a cycle training strategy to supervise the model in generating continuous EV LDR images without corresponding ground truths. Our CEVR model outperforms existing methods, as demonstrated by experimental results.
                </p>
            </div>
        </div>


<br>
        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <video id="v0" width="100%" autoplay="" loop="" muted="" controls="">
                  <source src="img/DemoVideo.mp4" type="video/mp4">
                </video>
            </div>
        </div> -->
        <div class="row">
          <div class="col-md-8 col-md-offset-2">
              <h3>
                  Video
              </h3>
              <div class="text-center">
                  <div style="position:relative;padding-top:56.25%;">
                      <iframe src="https://www.youtube.com/embed/Az8W2lGegcg" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                  </div>
              </div>
          </div>
      </div>
<br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Continuous LDR stack
                </h3>
                <br>
                <image src="img/continuous.jpg" style="width:100%;" class="img-responsive center-block" alt="overview"></image>
                <br>
                <p class="text-justify">
                  Traditional indirect methods use LDR images with predefined exposure values to construct the stack. In contrast, our approach explores continuous exposure value information, constructing a denser stack that includes LDR images with previously unseen continuous exposure values. We call this denser stack the continuous LDR stack. Compared to existing indirect methods, our approach, the continuous LDR stack, has the ability to recover a more accurate inverse camera response function, therefore enhancing the quality of the final HDR result.
                </p>
            </div>
        </div>

<br>


        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Continuous Exposure Value Representation (CEVR)
                </h3>
				<!-- <table style="width: 100%; border-collapse: collapse;"> -->
				  <tr>
				    <td style="text-align: center;">
		                <video id="v0" width="100%" autoplay loop muted>
		                  <source src="img/cevr.mp4" type="video/mp4" />
		                </video>
					</td>
				  </tr>
				<!-- </table> -->
                <p class="text-justify">
                  In order to generate the continuous LDR stack, we propose the CEVR model, which can generate LDR images with arbitrary exposure values. The generated LDR images with continuous exposure values are used to construct the continuous LDR stack.
                </p>
            </div>
        </div>
<br>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Cycle training
                </h3>
                <br>
                <image src="img/cycle.jpg" style="width:100%;" class="img-responsive center-block" alt="overview"></image>
                <br>
                <p class="text-justify">
                  For those exposure values without corresponding ground truth, how can CEVR understand the meaning of unseen exposure values during the training phase and generate a reasonable result? To address this challenge, we designed a novel training strategy called cycle training. This strategy involves two branches: one follows the conventional training approach, while the other branch divides the target exposure value into two incremental steps. These divided values are then processed through the CEVR model twice, and the sum of these two steps equals the target exposure value. Through this approach, our model learns explicit insights into exposure value variations and gets supervisory signals for exposure values without ground truth.
                </p>
            </div>
        </div>

<br>

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    LDR images generation with arbitrary EVs
                </h3>
                <p class="text-justify">
                CEVR visual results on the VDS and HDREye dataset!!
                </p>
                    <div class="image-container_result">
                          <img src="img/Result_Website_LDR.png" class="responsive"
                          onmouseover="changeImage(1,this)"
                          onmouseout="changeImage(2,this)">
                    </div>
                </p>
            </div>
        </div>
<br> -->
        <!-- <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h3>
                    Evaluation Result
                    </h3>
                    <p class="text-justify">
                    The ZIP file contain "results of each related work" and "the matlab script for evaluation". You can simply download the ZIP file and run the script to get the PSNR, HDR-VDP-2 score for tonemapped image and HDR file. (We evaluate on two datasets, VDS dataset and HDREye dataset, which are also in the ZIP file.) 
                    </p>
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://drive.google.com/file/d/1xeCT3APYkTnxeotb_t0wxdSPzBLbnU_p/view?usp=sharing">
                            <image src="img/ZIPImage.png" height="60px">
                                <h4><strong>Data and script</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div> -->


        

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Evaluation results (3.3G)
                </h3>
                <!-- <p class="text-justify">
                CEVR visual results on the VDS and HDREye dataset!!
                </p> -->
                <br>
                    <div class="one" onmouseout="ccc_stop()" onmouseover="ccc_start()">
                      <div class="two" id='ccc_image'><a href="https://drive.google.com/file/d/1xeCT3APYkTnxeotb_t0wxdSPzBLbnU_p/view?usp=sharing"><img src='img/Result_Website_HDR.png' class="responsive"></a></div>
                      <a href="https://drive.google.com/file/d/1xeCT3APYkTnxeotb_t0wxdSPzBLbnU_p/view?usp=sharing"><img src='img/Result_Website_LDR.png' class="responsive"></a>
                    </div>
                    <script type="text/javascript">
                      function ccc_start() {
                        document.getElementById('ccc_image').style.opacity = "1";
                      }

                      function ccc_stop() {
                        document.getElementById('ccc_image').style.opacity = "0";
                      }
                      ccc_stop()
                    </script>
                    <br>
                  <p class="text-justify">
                    We provide a <a href="https://drive.google.com/file/d/1xeCT3APYkTnxeotb_t0wxdSPzBLbnU_p/view?usp=sharing">ZIP file</a> containing each compared method's results and the Matlab script for the evaluation of two datasets: the VDS dataset and the HDREye dataset. You can download the ZIP file and run the script to get the PSNR, HDR-VDP-2 score for the tone-mapped image, and HDR file.
                  </p>
                </p>
            </div>
        </div>


<br>
        <div class="section">
          <!-- Icon Section -->
          <div class="row center">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Quantitative results
                </h3>
<!--                   <div class="image-container">
                    <img class="responsive-img" src="img/CEVR_Teaser.png" alt="Image Description">
                  </div> -->
                <!-- <div class="table-container"> -->
                    <!-- <table class="tg"> -->
                  <div class="table_wrapper">
                    <table class="table is-hoverable is-bordered">
                        <thead>
                          <tr>
                            <th class="tg-7s0l" rowspan="3">Dataset</th>
                            <th class="tg-7s0l" rowspan="3">Method</th>
                            <th class="tg-7s0l" colspan="2">PSNR</th>
                            <th class="tg-7s0l" colspan="2">PSNR</th>
                            <th class="tg-7s0l" colspan="2" rowspan="2">HDR-VDP-2</th>
                          </tr>
                          <tr>
                            <th class="tg-7s0l" colspan="2">RH's TMO</th>
                            <th class="tg-7s0l" colspan="2">KK's TMO</th>
                          </tr>
                          <tr>
                            <th class="tg-7s0l">mean</th>
                            <th class="tg-7s0l">std</th>
                            <th class="tg-7s0l">mean</th>
                            <th class="tg-7s0l">std</th>
                            <th class="tg-7s0l">mean</th>
                            <th class="tg-7s0l">std</th>
                          </tr>
                        </thead>
                        <tbody>
                          <tr>
                            <td class="tg-jb0k" rowspan="6">VDS</td>
                            <td class="tg-jb0k">DrTMO</td>
                            <td class="tg-jb0k">25.49</td>
                            <td class="tg-jb0k">4.28</td>
                            <td class="tg-jb0k">21.36</td>
                            <td class="tg-jb0k">4.50</td>
                            <td class="tg-jb0k">54.33</td>
                            <td class="tg-jb0k">6.27</td>
                          </tr>
                          <tr>
                            <td class="tg-jb0k">Deep chain HDRi</td>
                            <td class="tg-jb0k">30.86</td>
                            <td class="tg-jb0k">3.36</td>
                            <td class="tg-jb0k">24.54</td>
                            <td class="tg-jb0k">4.01</td>
                            <td class="tg-jb0k">56.36</td>
                            <td class="tg-jb0k">4.41</td>
                          </tr>
                          <tr>
                            <td class="tg-jb0k">Deep recursive HDRI</td>
                            <td class="tg-jb0k">32.99</td>
                            <td class="tg-jb0k">2.81</td>
                            <td class="tg-jb0k">28.02</td>
                            <td class="tg-jb0k">3.50</td>
                            <td class="tg-jb0k">57.15</td>
                            <td class="tg-jb0k">4.35</td>
                          </tr>
                          <tr>
                            <td class="tg-jb0k">Santos et al.</td>
                            <td class="tg-jb0k">22.56</td>
                            <td class="tg-jb0k">2.68</td>
                            <td class="tg-jb0k">18.23</td>
                            <td class="tg-jb0k">3.53</td>
                            <td class="tg-jb0k">53.51</td>
                            <td class="tg-jb0k">4.76</td>
                          </tr>
                          <tr>
                            <td class="tg-jb0k">Liu et al.</td>
                            <td class="tg-jb0k">30.89</td>
                            <td class="tg-jb0k">3.27</td>
                            <td class="tg-jb0k">28.00</td>
                            <td class="tg-jb0k">4.11</td>
                            <td class="tg-jb0k">56.97</td>
                            <td class="tg-jb0k">6.15</td>
                          </tr>
                          <tr>
                            <td class="tg-jb0k">CEVR (Ours)</td>
                            <td class="tg-eiei"><p style="color:red;"><b>34.67</b></p></td>
                            <td class="tg-jb0k">3.50</td>
                            <td class="tg-eiei"><p style="color:red;"><b>30.04</b></p></td>
                            <td class="tg-jb0k">4.45</td>
                            <td class="tg-eiei"><p style="color:red;"><b>59.00</b></p></td>
                            <td class="tg-jb0k">5.78</td>
                          </tr>
                          <tr>
                            <td class="tg-jb0k" rowspan="6">HDREye</td>
                            <td class="tg-jb0k">DrTMO</td>
                            <td class="tg-jb0k">23.68</td>
                            <td class="tg-jb0k">3.27</td>
                            <td class="tg-jb0k">19.97</td>
                            <td class="tg-jb0k">4.11</td>
                            <td class="tg-jb0k">46.67</td>
                            <td class="tg-jb0k">5.81</td>
                          </tr>
                          <tr>
                            <td class="tg-jb0k">Deep chain HDRi</td>
                            <td class="tg-jb0k">25.77</td>
                            <td class="tg-jb0k">2.44</td>
                            <td class="tg-jb0k">22.62</td>
                            <td class="tg-jb0k">3.39</td>
                            <td class="tg-jb0k">49.80</td>
                            <td class="tg-jb0k">5.97</td>
                          </tr>
                          <tr>
                            <td class="tg-jb0k">Deep recursive HDRI</td>
                            <td class="tg-jb0k">26.28</td>
                            <td class="tg-jb0k">2.70</td>
                            <td class="tg-jb0k">24.62</td>
                            <td class="tg-jb0k">2.90</td>
                            <td class="tg-jb0k">52.63</td>
                            <td class="tg-jb0k">4.84</td>
                          </tr>
                          <tr>
                            <td class="tg-jb0k">Santos et al.</td>
                            <td class="tg-jb0k">19.89</td>
                            <td class="tg-jb0k">2.46</td>
                            <td class="tg-jb0k">19.00</td>
                            <td class="tg-jb0k">3.06</td>
                            <td class="tg-jb0k">49.97</td>
                            <td class="tg-jb0k">5.44</td>
                          </tr>
                          <tr>
                            <td class="tg-jb0k">Liu et al.</td>
                            <td class="tg-jb0k">26.25</td>
                            <td class="tg-jb0k">3.08</td>
                            <td class="tg-jb0k">24.67</td>
                            <td class="tg-jb0k">3.54</td>
                            <td class="tg-jb0k">50.33</td>
                            <td class="tg-jb0k">6.67</td>
                          </tr>
                          <tr>
                            <td class="tg-jb0k">CEVR (Ours)</td>
                            <td class="tg-eiei"><p style="color:red;"><b>26.54</b></p></td>
                            <td class="tg-jb0k">3.10</td>
                            <td class="tg-eiei"><p style="color:red;"><b>24.81</b></p></td>
                            <td class="tg-jb0k">2.91</td>
                            <td class="tg-eiei"><p style="color:red;"><b>53.15</b></p></td>
                            <td class="tg-jb0k">4.91</td>
                          </tr>
                        </tbody>
                        </table>
                    </div>
                    <p class="text-justify">
                      Compared to other existing methods, our work demonstrates competitive performance.
                    </p>
                </div>
              </div>
        </div>

<br>
        <div class="row">
            <div class="col-md-50 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <!-- <div class="form-group col-md-30 col-md-offset-1"> -->
                <div class="form-group col-md-30 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@inproceedings{cevr_2023,
  title={CEVR: Learning Continuous Exposure Value Representations for Single-Image HDR Reconstruction},
  author={Chen, Su-Kai and Yen, Hung-Lin and Liu, Yu-Lun and Chen, Min-Hung and Hu, Hou-Ning and Peng, Wen-Hsiao and Lin, Yen-Yu},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  year={2023}
}</textarea>
                </div>
            </div>
        </div>
        <br>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                This work was supported in part by National Science and Technology Council (NSTC) under grants 111-2628-E-A49-025-MY3, 112-2221-E-A49-090-MY3, 111-2634-F-002-023, 111-2634-F-006-012, 110-2221-E-A49-065-MY3 and 111-2634-F-A49-010. This work was funded in part by MediaTek.
                    <br><br>
                The website template was borrowed from <a href="https://jonbarron.info/zipnerf/">Zip-NeRF</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
